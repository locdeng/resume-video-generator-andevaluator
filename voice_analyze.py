import streamlit as st
import tempfile
import librosa
import numpy as np
from utils.audio_preprocessing import extract_audio, preprocess_audio
from utils.transcription import load_whisper, split_audio_chunks
from utils.audio_analysis import (
    analyze_speech_rate, count_filler_words, sentence_length,
    detect_pauses, measure_loudness, analyze_pitch
)
from dotenv import load_dotenv
import os
import google.generativeai as genai
from about_me_gen import generate_text_gemini
from utils.gemini_utils import make_prompt_from_features

# Load .env
load_dotenv()
api_key = os.getenv("GOOGLE_API_KEY")
genai.configure(api_key=api_key)

# Load Whisper once (global to avoid reloading)
asr = load_whisper()

def run_transcription_app():
    st.markdown("""
        <style>
        
         @font-face {
            font-family: 'SB_B';
            src: url('assets/fonts/SF.ttf') format('truetype');
        }
        
                /* ToÃ n bá»™ trang (ná»n Ä‘en) */
        html, body {
            background-color: #f0e8db !important;
            font-family: 'SF',sans-serif;
        }

        /* Ná»n vÃ¹ng ná»™i dung */
        [data-testid="stAppViewContainer"] {
            background-color: #f0e8db !important;
        }

        /* Ná»n container chÃ­nh */
        [data-testid="stAppViewBlockContainer"] {
            background-color: #f0e8db !important;
            padding: 0rem 1rem; /* giáº£m padding náº¿u muá»‘n */
            max-width: 100% !important;  /* full width */
        }

        /* Optional: Sidebar náº¿u báº¡n muá»‘n cÅ©ng ná»n Ä‘en */
        [data-testid="stSidebar"] {
            background-color: #77C9D4 !important;
        }
        .intro-title {
            font-size: 48px;
            font-weight: 800;
            color: #2b2b2b;
            text-align: center;
            font-family: 'SF',sans-serif;
            margin-top: 30px;
        }
        .intro-sub {
            font-size: 18px;
            color: #2b2b2b;
            text-align: center;
            font-family: 'SF',sans-serif;
            margin-top: -10px;
            margin-bottom: 30px;
        }
        .feature-box {
            background: #F2EFE7 ;
            padding: 30px;
            border-radius: 15px;
            margin: 10px 20px;
            color: #2b2b2b;
            border: 2px solid white;
            font-family: 'SF',sans-serif;
            text-align: center;
        }
        .feature-title {
            font-size: 22px;
            font-weight: bold;
            margin-bottom: 10px;
            font-family: 'SF',sans-serif;
            color: #2b2b2b;
        }
        .stButton>button {
            width: 100%;
            border-radius: 10px;
            padding: 12px;
            font-size: 16px;
            font-weight: bold;
            background-color: #F2EFE7;
            border: 2px solid white;
            color: #2b2b2b;
        }
        </style>
    """, unsafe_allow_html=True)
    
    """
    Streamlit App UI for Korean Speech Transcription and Analysis
    """
    # st.title("ğŸ™ï¸ Whisperë¥¼ í™œìš©í•œ í•œêµ­ì–´ ìŒì„± ì „ì‚¬")
    st.markdown('<div class="intro-title">ë™ì˜ìƒ ìŒì„± ë¶„ì„</div>', unsafe_allow_html=True)
    st.markdown("""
    - í•œêµ­ì–´ì—ë§Œ ì ìš©
    - ë¹„ë””ì˜¤ (.mp4) ë˜ëŠ” ì˜¤ë””ì˜¤ (.wav/.mp3) ì—…ë¡œë“œ  
    - ì˜¤ë””ì˜¤ ìë™ ì¶”ì¶œ  
    - ë…¸ì´ì¦ˆ ì œê±° + ë³¼ë¥¨ ì •ê·œí™”  
    - ê¸´ ì˜¤ë””ì˜¤ë¥¼ ìë™ìœ¼ë¡œ 30ì´ˆ ì²­í¬ë¡œ ë¶„í•   
    - Whisper ëª¨ë¸ë¡œ í•œêµ­ì–´ ì „ì‚¬
    """)

    uploaded_file = st.file_uploader("ğŸ“¤ ë¹„ë””ì˜¤ ë˜ëŠ” ì˜¤ë””ì˜¤ ì—…ë¡œë“œ", type=["mp4", "wav", "mp3"])

    if uploaded_file is not None:
        with tempfile.NamedTemporaryFile(delete=False) as tmp_in:
            tmp_in.write(uploaded_file.read())
            input_path = tmp_in.name

        st.success(f"âœ… Uploaded: {uploaded_file.name}")

        # STEP 1: Extract audio if video
        if uploaded_file.type.startswith("video/"):
            st.info("ğŸ”„ ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘...")
            raw_audio_path = tempfile.NamedTemporaryFile(delete=False, suffix=".wav").name
            extract_audio(input_path, raw_audio_path)
            st.success("ğŸµ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì™„ë£Œ!")
        else:
            raw_audio_path = input_path

        # STEP 2: Preprocess audio
        st.info("âœ¨ Preprocessing (Noise Reduction + Normalize)...")
        preprocessed_audio_path = tempfile.NamedTemporaryFile(delete=False, suffix=".wav").name
        preprocess_audio(raw_audio_path, preprocessed_audio_path)
        st.success("âœ… Audio preprocessed!")

        # STEP 3: Transcription
        if st.button("ğŸ“ Transcribe Now"):
            with st.spinner("â³ ì˜¤ë””ì˜¤ë¥¼ 30ì´ˆ ì²­í¬ë¡œ ë¶„í•  ì¤‘..."):
                chunk_files = split_audio_chunks(preprocessed_audio_path)
                st.success(f"âœ… {len(chunk_files)} ê°œì˜ ì²­í¬ ìƒì„± ì™„ë£Œ.")

            full_transcript = ""
            all_chunks = []
            total_audio_length = 0
            all_pauses = []
            all_loudness = []
            all_pitches = []
            offset = 0

            speech_rates = []
            filler_counts = []
            num_sentences_all = []
            avg_sentence_lengths = []

            for idx, chunk_file in enumerate(chunk_files, 1):
                st.info(f"ğŸ”Š Transcribing chunk {idx}/{len(chunk_files)}...")

                result = asr(
                    chunk_file,
                    generate_kwargs={"language": "korean"},
                    return_timestamps=True
                )

                full_transcript += result["text"].strip() + "\n\n"
                duration = librosa.get_duration(filename=chunk_file)
                total_audio_length += duration

                speech_rate = analyze_speech_rate(result["text"], duration)
                filler_count = count_filler_words(result["text"])
                num_sentences, avg_sentence_length = sentence_length(result["text"])

                speech_rates.append(speech_rate)
                filler_counts.append(filler_count)
                num_sentences_all.append(num_sentences)
                avg_sentence_lengths.append(avg_sentence_length)

                clean_chunks = []
                for chunk in result["chunks"]:
                    start, end = chunk["timestamp"]
                    text = chunk["text"].strip()
                    if start is not None and end is not None and text:
                        clean_chunks.append({
                            "start": start + offset,
                            "end": end + offset,
                            "text": text
                        })

                all_chunks.extend(clean_chunks)
                offset += duration

                pauses = detect_pauses(chunk_file)
                loudness = measure_loudness(chunk_file)
                pitch_mean, pitch_variation = analyze_pitch(chunk_file)

                all_pauses.extend(pauses)
                all_loudness.append(loudness)
                all_pitches.append((pitch_mean, pitch_variation))

                st.write(f"âœ… ë§ ì†ë„: {speech_rate:.2f} ë‹¨ì–´/ë¶„")
                st.write(f"âœ… ë©”ìš°ëŠ” ë§ íšŸìˆ˜: {filler_count}")
                st.write(f"âœ… ë¬¸ì¥ ìˆ˜: {num_sentences}, í‰ê·  ë¬¸ì¥ ê¸¸ì´: {avg_sentence_length:.2f}")
                st.write(f"âœ… ë©ˆì¶¤: {len(pauses)}íšŒ")
                st.write(f"âœ… í‰ê·  ë³¼ë¥¨: {loudness:.2f} dBFS")
                st.write(f"âœ… ìŒë†’ì´ (í‰ê· , ë³€ë™): {pitch_mean:.2f} Hz, {pitch_variation:.2f} Hz")

            # ìš”ì•½
            total_pauses = len(all_pauses)
            avg_speech_rate = np.mean(speech_rates)
            avg_filler_count = np.sum(filler_counts)
            avg_loudness = np.mean(all_loudness)
            avg_pitch_mean = np.mean([p[0] for p in all_pitches if p[0] is not None])
            avg_pitch_variation = np.mean([p[1] for p in all_pitches if p[1] is not None])

            st.subheader("ğŸ” ì „ì²´ ì˜¤ë””ì˜¤ ë¶„ì„ ìš”ì•½")
            st.write(f"âœ… ì´ ë©ˆì¶¤ íšŸìˆ˜: {total_pauses}")
            st.write(f"âœ… ë§ ì†ë„ í‰ê· : {avg_speech_rate:.2f} ë‹¨ì–´/ë¶„")
            st.write(f"âœ… ì´ ë©”ìš°ëŠ” ë§ íšŸìˆ˜: {avg_filler_count}")
            st.write(f"âœ… í‰ê·  ë³¼ë¥¨: {avg_loudness:.2f} dBFS")
            st.write(f"âœ… ìŒë†’ì´ í‰ê· : {avg_pitch_mean:.2f} Hz")
            st.write(f"âœ… ìŒë†’ì´ ë³€ë™: {avg_pitch_variation:.2f} Hz")

            st.subheader("ğŸ“œ Full Transcript (Korean)")
            st.text_area("ì „ì²´ ìŠ¤í¬ë¦½íŠ¸:", full_transcript.strip(), height=300)

            st.subheader("ğŸ•’ Segmented with Timestamps")
            for c in all_chunks:
                st.markdown(f"**[{c['start']:.1f}s - {c['end']:.1f}s]**: {c['text']}")

            st.download_button(
                "ğŸ’¾ Download Transcript",
                data=full_transcript,
                file_name="transcript.txt",
                mime="text/plain"
            )

            # Save results to session_state
            st.session_state["analysis_result"] = {
                "speech_rate": avg_speech_rate,
                "filler_count": avg_filler_count,
                "num_pauses": total_pauses,
                "loudness": avg_loudness,
                "pitch_mean": avg_pitch_mean,
                "pitch_variation": avg_pitch_variation,
                "transcript": full_transcript
            }

        # AI Insight button
        if "analysis_result" in st.session_state:
            if st.button("âœ¨ AI ì¸ì‚¬ì´íŠ¸ ìš”ì²­"):
                with st.spinner("ğŸ” ë¬¼ì–´ë³´ê³  ìˆëŠ”ì¤‘..."):
                    prompt = make_prompt_from_features(st.session_state["analysis_result"])
                    result = generate_text_gemini(prompt)
                st.subheader("ğŸ“ˆ AI Insight")
                st.write(result)
